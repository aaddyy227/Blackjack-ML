{
  "cells": [
    {
      "metadata": {
        "_uuid": "545b22cc7e3824c2d6b19932959e91307f51825c",
        "id": "NdM3Et5MIc8X"
      },
      "cell_type": "markdown",
      "source": [
        "# **Optimising Blackjack Strategy using Model-Free Learning**\n",
        "\n",
        "In Reinforcement learning, there are 2 kinds of approaches, model-based learning and model-free learning. Model-Based Learning can be applied if we have full information of the transition probabilitiies and rewards, but it would be too computationally expensive if the game gets too complex.\n",
        "\n",
        "Model-Free Learning is the more practical approach as it doesn't need to have information on the full transition probabilities and rewards as it focus on figuring out value function directly from the interactions with the environment.\n",
        "\n",
        "We would attempt to train an agent to play blackjack using model-free learning approach."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5IlEd8UI20p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9a841496b30aaf62f50e03ed13df8ac82dc6f3b0",
        "id": "xu2R4skmIc8Z"
      },
      "cell_type": "markdown",
      "source": [
        "**1. Basics of the OpenAi Blackjack Environment**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "04998cf817d06fbd97532992a34b0fdcfe5948d4",
        "id": "WNCraEgPIc8Z"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "1e3cb6a9728eeb07a1c7035d5e416ce52c5dffc9",
        "id": "WDEY6UYPIc8Z"
      },
      "cell_type": "markdown",
      "source": [
        "The states are stored in this tuple format:\n",
        "\n",
        "(Agent's score , Dealer's visible score, and whether or not the agent has a usable ace)"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "9ef9098d22563c700d1be1bcb75a61e7fa75dd32",
        "id": "dWYjZInwIc8a"
      },
      "cell_type": "code",
      "source": [
        "env.observation_space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7727bea1832a51227675c9c024cd10a92c969bec",
        "id": "yPAP-O5IIc8a"
      },
      "cell_type": "markdown",
      "source": [
        "The agent only has 2 options: Hit(1) or Stay(0)."
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "f9cb2bb92ddf8eb2a86d91f7b9066257aef1db39",
        "id": "p_SQLVU7Ic8a"
      },
      "cell_type": "code",
      "source": [
        "env.action_space.n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "474426e96b5cf4e363bcc2c652e8ea052dd82db7",
        "id": "XJKM81cEIc8a"
      },
      "cell_type": "markdown",
      "source": [
        "Let's view one scenario."
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "4ccc4e342e89655efd6560c9e2fc21af0a6278cd",
        "id": "j3EGZFiGIc8a"
      },
      "cell_type": "code",
      "source": [
        "env.reset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "f773aad6b4a2676abfbbf46e5679cbd1de753ff1",
        "id": "cQYgQXeyIc8a"
      },
      "cell_type": "code",
      "source": [
        "visible = \"\" if env._get_obs()[2] else \"no\"\n",
        "print(\"The above shows that the player's hand has a total sum of \"\n",
        "      + str(env._get_obs()[0]) + \" while the dealer visible hand is \"\n",
        "      + str(env._get_obs()[1]) + \" and that the player has \" + visible + \" usable ace\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "551241ad94241f107ffab5141f355bbeb199e275",
        "id": "-KId6ZPJIc8a"
      },
      "cell_type": "markdown",
      "source": [
        "Let's assume the player's first action would be to \"hit\" (Action step would be 1). The following would occur."
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "b482d2e5a867a4a0456c73d08d91689c202f90c4",
        "id": "II761JB5Ic8a"
      },
      "cell_type": "code",
      "source": [
        "env.step(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "95457e2920ca6f2478c218beccdb577668318824",
        "id": "fjvK9u_pIc8b"
      },
      "cell_type": "markdown",
      "source": [
        "Assume the next action would be to stay."
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "d55ec7c443bc13600ff699e5f584d98ccd0a54f9",
        "id": "wCtqoTAUIc8b"
      },
      "cell_type": "code",
      "source": [
        "env.step(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "75c347c0a8733aadd922dfe8124ed7b183c74618",
        "id": "FczoaXnBIc8b"
      },
      "cell_type": "markdown",
      "source": [
        "The second element in the tuple above shows the reward. The reward for winning is +1, drawing is 0, and losing is -1. We can also see the actual dealer hand in:"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "c0bdef153e74a8e13d701d426b426f4a5ac13d05",
        "id": "fEKrHdOmIc8b"
      },
      "cell_type": "code",
      "source": [
        "env.dealer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4850780f4feaf8892626ffa0ad9d2da7eea8f32a",
        "id": "ViD6DUC-Ic8b"
      },
      "cell_type": "markdown",
      "source": [
        "The current Dealer strategy is to draw cards as long as he has a score of below 17."
      ]
    },
    {
      "metadata": {
        "_uuid": "40671327d0f3aa0de43be99c412c4a209a5a95b7",
        "id": "JQ2ImrJBIc8b"
      },
      "cell_type": "markdown",
      "source": [
        "**2. Basic Naive Strategy**\n",
        "\n",
        "Let's start by testing out a basic strategy which is to **Draw as long as the score is below 17** and calculate the average payoff."
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "00a01bec8b10827a8c77df476980caedbf6e68b0",
        "id": "3ErS7cebIc8b"
      },
      "cell_type": "code",
      "source": [
        "def draw_till_17_pol(obs):\n",
        "    return [1,0] if obs[0]<17 else [0,1]\n",
        "\n",
        "def calc_payoffs(env,rounds,players,pol):\n",
        "    \"\"\"\n",
        "    Calculate Payoffs.\n",
        "\n",
        "    Args:\n",
        "        env: environment\n",
        "        rounds: Number of rounds a player would play\n",
        "        players: Number of players\n",
        "        pol: Policy used\n",
        "\n",
        "    Returns:\n",
        "        Average payoff\n",
        "    \"\"\"\n",
        "    average_payouts = []\n",
        "    for player in range(players):\n",
        "        rd = 1\n",
        "        total_payout = 0 # to store total payout over 'num_rounds'\n",
        "\n",
        "        while rd <= rounds:\n",
        "            action = np.argmax(pol(env._get_obs()))\n",
        "            obs, payout, is_done, _ = env.step(action)\n",
        "            if is_done:\n",
        "                total_payout += payout\n",
        "                env.reset() # Environment deals new cards to player and dealer\n",
        "                rd += 1\n",
        "        average_payouts.append(total_payout)\n",
        "\n",
        "    plt.plot(average_payouts)\n",
        "    plt.xlabel('num_player')\n",
        "    plt.ylabel('payout after ' + str(rounds) + 'rounds')\n",
        "    plt.show()\n",
        "    print (\"Average payout of a player after {} rounds is {}\".format(rounds, sum(average_payouts)/players))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "674cb6ffb2aa654fa3972f8fc57ef8b0cc1e40c1",
        "id": "muoL6snvIc8b"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v0')\n",
        "env.reset()\n",
        "calc_payoffs(env,1000,1000,draw_till_17_pol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b6988557bb00692f5effca54ff63d4bbf541cf0a",
        "id": "5EzEldFfIc8b"
      },
      "cell_type": "markdown",
      "source": [
        "**3. Monte Carlo Method**"
      ]
    },
    {
      "metadata": {
        "_uuid": "fae91a9d7518fe9b4fe9cc96b0ff3b74f0498539",
        "id": "8vZxbtX5Ic8b"
      },
      "cell_type": "markdown",
      "source": [
        "Let's first define some function to plot our policy and value function."
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "35a618dfdc53999cfc52a5e02c78e27f5598aca9",
        "id": "lkcha9eFIc8b"
      },
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "def plot_policy(policy):\n",
        "\n",
        "    def get_Z(player_hand, dealer_showing, usable_ace):\n",
        "        if (player_hand, dealer_showing, usable_ace) in policy:\n",
        "            return policy[player_hand, dealer_showing, usable_ace]\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    def get_figure(usable_ace, ax):\n",
        "        x_range = np.arange(1, 11)\n",
        "        y_range = np.arange(11, 22)\n",
        "        X, Y = np.meshgrid(x_range, y_range)\n",
        "        Z = np.array([[get_Z(player_hand, dealer_showing, usable_ace) for dealer_showing in x_range] for player_hand in range(21, 10, -1)])\n",
        "        surf = ax.imshow(Z, cmap=plt.get_cmap('Accent', 2), vmin=0, vmax=1, extent=[0.5, 10.5, 10.5, 21.5])\n",
        "        plt.xticks(x_range, ('A', '2', '3', '4', '5', '6', '7', '8', '9', '10'))\n",
        "        plt.yticks(y_range)\n",
        "        ax.set_xlabel('Dealer Showing')\n",
        "        ax.set_ylabel('Player Hand')\n",
        "        ax.grid(color='black', linestyle='-', linewidth=1)\n",
        "        divider = make_axes_locatable(ax)\n",
        "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
        "        cbar = plt.colorbar(surf, ticks=[0, 1], cax=cax)\n",
        "        cbar.ax.set_yticklabels(['0 (STICK)','1 (HIT)'])\n",
        "        cbar.ax.invert_yaxis()\n",
        "\n",
        "    fig = plt.figure(figsize=(12, 12))\n",
        "    ax = fig.add_subplot(121)\n",
        "    ax.set_title('Usable Ace', fontsize=16)\n",
        "    get_figure(True, ax)\n",
        "    ax = fig.add_subplot(122)\n",
        "    ax.set_title('No Usable Ace', fontsize=16)\n",
        "    get_figure(False, ax)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "8f9abac0d3987c45bb987dc70753e26d2c1cb74d",
        "id": "h54MQjs6Ic8b"
      },
      "cell_type": "code",
      "source": [
        "def plot_value_function(V, title=\"Value Function\"):\n",
        "    \"\"\"\n",
        "    Plots the value function as a surface plot.\n",
        "    \"\"\"\n",
        "    min_x = min(k[0] for k in V.keys())\n",
        "    max_x = max(k[0] for k in V.keys())\n",
        "    min_y = min(k[1] for k in V.keys())\n",
        "    max_y = max(k[1] for k in V.keys())\n",
        "\n",
        "    x_range = np.arange(min_x, max_x + 1)\n",
        "    y_range = np.arange(min_y, max_y + 1)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "\n",
        "    # Find value for all (x, y) coordinates\n",
        "    Z_noace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))\n",
        "    Z_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))\n",
        "\n",
        "    def plot_surface(X, Y, Z, title):\n",
        "        fig = plt.figure(figsize=(16,8))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "                               cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "        ax.set_xlabel('Player Sum')\n",
        "        ax.set_ylabel('Dealer Showing')\n",
        "        ax.set_zlabel('Value')\n",
        "        ax.set_title(title)\n",
        "        ax.view_init(ax.elev, -120)\n",
        "        fig.colorbar(surf)\n",
        "        plt.show()\n",
        "\n",
        "    plot_surface(X, Y, Z_noace, \"{} (No Usable Ace)\".format(title))\n",
        "    plot_surface(X, Y, Z_ace, \"{} (Usable Ace)\".format(title))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "393a5cd0bcd8985a3bc871218d54bd1c21b4b684",
        "id": "BaS7unAwIc8b"
      },
      "cell_type": "markdown",
      "source": [
        "**3.1 MC (On-Policy) --> Learn from the top**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "3cda22672fc15d2e20f6f513687108383a385206",
        "id": "hCn5e4lNIc8c"
      },
      "cell_type": "code",
      "source": [
        "def create_epsilon_greedy_action_policy(env,Q,epsilon):\n",
        "    \"\"\" Create epsilon greedy action policy\n",
        "    Args:\n",
        "        env: Environment\n",
        "        Q: Q table\n",
        "        epsilon: Probability of selecting random action instead of the 'optimal' action\n",
        "\n",
        "    Returns:\n",
        "        Epsilon-greedy-action Policy function with Probabilities of each action for each state\n",
        "    \"\"\"\n",
        "    def policy(obs):\n",
        "        P = np.ones(env.action_space.n, dtype=float) * epsilon / env.action_space.n  #initiate with same prob for all actions\n",
        "        best_action = np.argmax(Q[obs])  #get best action\n",
        "        P[best_action] += (1.0 - epsilon)\n",
        "        return P\n",
        "    return policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "999c38ade56b7cea62859bbd20fcb2e04fc4fe7d",
        "id": "GvpIeXaZIc8c"
      },
      "cell_type": "code",
      "source": [
        "def On_pol_mc_control_learn(env, episodes, discount_factor, epsilon):\n",
        "    \"\"\"\n",
        "    Monte Carlo Control using Epsilon-Greedy policies.\n",
        "    Finds an optimal epsilon-greedy policy.\n",
        "\n",
        "    Args:\n",
        "        env: Environment.\n",
        "        episodes: Number of episodes to sample.\n",
        "        discount_factor: Gamma discount factor.\n",
        "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
        "\n",
        "    Returns:\n",
        "        A tuple (Q, policy).\n",
        "        Q is a dictionary mapping state to action values.\n",
        "        Policy is the trained policy that returns action probabilities\n",
        "    \"\"\"\n",
        "    # Keeps track of sum and count of returns for each state\n",
        "    # An array could be used to save all returns but that's memory inefficient.\n",
        "    # defaultdict used so that the default value is stated if the observation(key) is not found\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "\n",
        "    # The final action-value function.\n",
        "    # A nested dictionary that maps state -> (action -> action-value).\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    # The policy we're following\n",
        "    pol = create_epsilon_greedy_action_policy(env,Q,epsilon)\n",
        "\n",
        "    for i in range(1, episodes + 1):\n",
        "        # Print out which episode we're on\n",
        "        if i% 1000 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i, episodes), end=\"\")\n",
        "            clear_output(wait=True)\n",
        "\n",
        "        # Generate an episode.\n",
        "        # An episode is an array of (state, action, reward) tuples\n",
        "        episode = []\n",
        "        state = env.reset()\n",
        "        for t in range(100):\n",
        "            probs = pol(state)\n",
        "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        # Find all (state, action) pairs we've visited in this episode\n",
        "        # We convert each state to a tuple so that we can use it as a dict key\n",
        "        sa_in_episode = set([(tuple(x[0]), x[1]) for x in episode])\n",
        "        for state, action in sa_in_episode:\n",
        "            sa_pair = (state, action)\n",
        "            #First Visit MC:\n",
        "            # Find the first occurance of the (state, action) pair in the episode\n",
        "            first_occurence_idx = next(i for i,x in enumerate(episode)\n",
        "                                       if x[0] == state and x[1] == action)\n",
        "            # Sum up all rewards since the first occurance\n",
        "            G = sum([x[2]*(discount_factor**i) for i,x in enumerate(episode[first_occurence_idx:])])\n",
        "            # Calculate average return for this state over all sampled episodes\n",
        "            returns_sum[sa_pair] += G\n",
        "            returns_count[sa_pair] += 1.0\n",
        "            Q[state][action] = returns_sum[sa_pair] / returns_count[sa_pair]\n",
        "\n",
        "    return Q, pol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "846235a029bd182defc73399a7e17f2034eac710",
        "id": "Eym8DVL_Ic8c"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v0')\n",
        "env.reset()\n",
        "Q_on_pol,On_MC_Learned_Policy = On_pol_mc_control_learn(env, 500000, 0.9, 0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "d3a5887dd62be33bb5fa1ed2f276f0dc79562f1c",
        "id": "jkP-12lrIc8c"
      },
      "cell_type": "code",
      "source": [
        "V = defaultdict(float)\n",
        "for state, actions in Q_on_pol.items():\n",
        "    action_value = np.max(actions)\n",
        "    V[state] = action_value\n",
        "plot_value_function(V, title=\"Optimal Value Function for On-Policy Learning\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "5a094531b2adcc240020b389a20930c64b9d18e9",
        "id": "3FFXZJcVIc8c"
      },
      "cell_type": "code",
      "source": [
        "on_pol = {key: np.argmax(On_MC_Learned_Policy(key)) for key in Q_on_pol.keys()}\n",
        "print(\"On-Policy MC Learning Policy\")\n",
        "plot_policy(on_pol)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "ce89bca37c60adecada009bceddaeeb288cfb22c",
        "id": "3OrHuMR-Ic8c"
      },
      "cell_type": "code",
      "source": [
        "#Payoff for On-Policy MC Trained Policy\n",
        "env.reset()\n",
        "calc_payoffs(env,1000,1000,On_MC_Learned_Policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d88f6fe33da010e40df4e56dc7f68f0c4a79bb4d",
        "id": "6cAmIcmfIc8c"
      },
      "cell_type": "markdown",
      "source": [
        "**3.2 MC (Off-Policy) --> Learn from the tail**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "54c6eca1a3f6a121427f35ef54f5dd93c8016a54",
        "id": "aXXET8koIc8c"
      },
      "cell_type": "code",
      "source": [
        "def create_random_policy(nA):\n",
        "    \"\"\"\n",
        "    Creates a random policy function.\n",
        "\n",
        "    Args:\n",
        "        nA: Number of actions in the environment.\n",
        "\n",
        "    Returns:\n",
        "        A function that takes an observation state as input and returns a vector\n",
        "        of action probabilities\n",
        "    \"\"\"\n",
        "    A = np.ones(nA, dtype=float) / nA\n",
        "    def policy_fn(obs):\n",
        "        return A\n",
        "    return policy_fn\n",
        "def create_greedy_action_policy(env,Q):\n",
        "    \"\"\" Create greedy action policy\n",
        "    Args:\n",
        "        env: Environment\n",
        "        Q: Q table\n",
        "\n",
        "    Returns:\n",
        "        Greedy-action Policy function\n",
        "    \"\"\"\n",
        "    def policy(obs):\n",
        "        P = np.zeros_like(Q[obs], dtype=float)\n",
        "        best_action = np.argmax(Q[obs])  #get best action\n",
        "        P[best_action] = 1\n",
        "        return P\n",
        "    return policy\n",
        "\n",
        "def Off_pol_mc_control_learn(env, num_episodes, policy, discount_factor):\n",
        "    \"\"\"\n",
        "    Monte Carlo Control Off-Policy Control using Weighted Importance Sampling.\n",
        "    Finds an optimal greedy policy.\n",
        "\n",
        "    Args:\n",
        "        env: Environment.\n",
        "        num_episodes: Number of episodes to sample.\n",
        "        policy: The policy to follow while generating episodes.\n",
        "            A function that given an observation returns a vector of probabilities for each action.\n",
        "        discount_factor: Gamma discount factor.\n",
        "\n",
        "    Returns:\n",
        "        A tuple (Q, policy).\n",
        "        Q is a dictionary mapping state -> action values.\n",
        "        policy is a function that takes an observation as an argument and returns\n",
        "        action probabilities. This is the optimal greedy policy.\n",
        "    \"\"\"\n",
        "\n",
        "    # The final action-value function.\n",
        "    # A dictionary that maps state -> action values\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    # The cumulative denominator of the weighted importance sampling formula\n",
        "    # (across all episodes)\n",
        "    C = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    # Our greedy policy\n",
        "    target_policy = create_greedy_action_policy(env,Q)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        if i_episode % 1000 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
        "            clear_output(wait=True)\n",
        "\n",
        "        # Generate an episode.\n",
        "        # An episode is an array of (state, action, reward) tuples\n",
        "        episode = []\n",
        "        state = env.reset()\n",
        "        for t in range(100):\n",
        "            # Sample an action from our policy\n",
        "            probs = target_policy(state)\n",
        "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        # Sum of discounted returns\n",
        "        G = 0.0\n",
        "        # The importance sampling ratio (the weights of the returns)\n",
        "        W = 1.0\n",
        "        # For each step in the episode, backwards\n",
        "        for t in range(len(episode))[::-1]:\n",
        "            state, action, reward = episode[t]\n",
        "            # Update the total reward since step t\n",
        "            G = discount_factor * G + reward\n",
        "            # Update weighted importance sampling formula denominator\n",
        "            C[state][action] += W\n",
        "            # Update the action-value function using the incremental update formula\n",
        "            # This also improves our target policy which holds a reference to Q\n",
        "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
        "            # If the action taken by the policy is not the action\n",
        "            # taken by the target policy the probability will be 0 and we can break\n",
        "            if action !=  np.argmax(target_policy(state)):\n",
        "                break\n",
        "            W = W * 1./policy(state)[action]\n",
        "\n",
        "    return Q, target_policy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "bccf3a45391d50ccbbb72d19874e2ba89581c218",
        "id": "ER0S_l6NIc8c"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v0')\n",
        "env.reset()\n",
        "rand = create_random_policy(env.action_space.n)\n",
        "Q_off_Pol,off_MC_Learned_Policy = Off_pol_mc_control_learn(env, 500000, rand,0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "bd64c6b86017a6beca2ab133260c2b6980c88ea4",
        "id": "9CJEDyv5Ic8c"
      },
      "cell_type": "code",
      "source": [
        "#Payoff for Off-Policy MC Trained Policy\n",
        "env.reset()\n",
        "calc_payoffs(env,1000,1000,off_MC_Learned_Policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "139ec1bebe3b9d87d72ebfb670a1c9270918457b",
        "id": "aAN97xjoIc8c"
      },
      "cell_type": "code",
      "source": [
        "V = defaultdict(float)\n",
        "for state, actions in Q_off_Pol.items():\n",
        "    action_value = np.max(actions)\n",
        "    V[state] = action_value\n",
        "plot_value_function(V, title=\"Optimal Value Function for Off-Policy Learning\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "d0d038ae108642645a05ef8084c3d9c7883309a7",
        "id": "O8BWD1OoIc8c"
      },
      "cell_type": "code",
      "source": [
        "pol_test = {key: np.argmax(off_MC_Learned_Policy(key)) for key in Q_off_Pol.keys()}\n",
        "print(\"Off-Policy MC Learning Policy\")\n",
        "plot_policy(pol_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "45b77e0547c70b3da50feef3667796102e6f4eae",
        "id": "qbt-CkYzIc8c"
      },
      "cell_type": "markdown",
      "source": [
        "**4. TD Learning**"
      ]
    },
    {
      "metadata": {
        "_uuid": "2c2351b0b5cf11504cd22414a82a4a05fd7c1ca0",
        "id": "lIwyNIKPIc8f"
      },
      "cell_type": "markdown",
      "source": [
        "**4.1 SARSA Learning**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "9d3775c14c80320b18d62ea7b4012cd748857916",
        "id": "Swc7YjXSIc8f"
      },
      "cell_type": "code",
      "source": [
        "def create_epsilon_greedy_action_policy(env,Q,epsilon):\n",
        "    \"\"\" Create epsilon greedy action policy\n",
        "    Args:\n",
        "        env: Environment\n",
        "        Q: Q table\n",
        "        epsilon: Probability of selecting random action instead of the 'optimal' action\n",
        "\n",
        "    Returns:\n",
        "        Epsilon-greedy-action Policy function with Probabilities of each action for each state\n",
        "    \"\"\"\n",
        "    def policy(obs):\n",
        "        P = np.ones(env.action_space.n, dtype=float) * epsilon / env.action_space.n  #initiate with same prob for all actions\n",
        "        best_action = np.argmax(Q[obs])  #get best action\n",
        "        P[best_action] += (1.0 - epsilon)\n",
        "        return P\n",
        "    return policy\n",
        "def SARSA(env, episodes, epsilon, alpha, gamma):\n",
        "    \"\"\"\n",
        "    SARSA Learning Method\n",
        "\n",
        "    Args:\n",
        "        env: OpenAI gym environment.\n",
        "        episodes: Number of episodes to sample.\n",
        "        epsilon: Probability of selecting random action instead of the 'optimal' action\n",
        "        alpha: Learning Rate\n",
        "        gamma: Gamma discount factor\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        A tuple (Q, policy).\n",
        "        Q is a dictionary mapping state -> action values.\n",
        "        policy is a function that takes an observation as an argument and returns\n",
        "        action probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialise a dictionary that maps state -> action values\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    # The policy we're following\n",
        "    pol = create_epsilon_greedy_action_policy(env,Q,epsilon)\n",
        "    for i in range(1, episodes + 1):\n",
        "        # Print out which episode we're on\n",
        "        if i% 1000 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i, episodes), end=\"\")\n",
        "            clear_output(wait=True)\n",
        "        curr_state = env.reset()\n",
        "        probs = pol(curr_state)   #get epsilon greedy policy\n",
        "        curr_act = np.random.choice(np.arange(len(probs)), p=probs)\n",
        "        while True:\n",
        "            next_state,reward,done,_ = env.step(curr_act)\n",
        "            next_probs = create_epsilon_greedy_action_policy(env,Q,epsilon)(next_state)\n",
        "            next_act = np.random.choice(np.arange(len(next_probs)),p=next_probs)\n",
        "            td_target = reward + gamma * Q[next_state][curr_act]\n",
        "            td_error = td_target - Q[curr_state][curr_act]\n",
        "            Q[curr_state][curr_act] = Q[curr_state][curr_act] + alpha * td_error\n",
        "            if done:\n",
        "                break\n",
        "            curr_state = next_state\n",
        "            curr_act = next_act\n",
        "    return Q, pol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "e67c0a75a03ad26f3dd0b619a45bf878f457e7f3",
        "id": "5BQNVzDaIc8f"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v0')\n",
        "env.reset()\n",
        "Q_SARSA,SARSA_Policy = SARSA(env, 500000, 0.1, 0.1,0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "aa4b5cf587818d8c6c74d0deefb63a7739525164",
        "id": "j_KBKVe8Ic8f"
      },
      "cell_type": "code",
      "source": [
        "#Payoff for Off-Policy MC Trained Policy\n",
        "env.reset()\n",
        "calc_payoffs(env,1000,1000,SARSA_Policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "b8f04cd97d2d2023bcc0a83d45772051bda53e95",
        "id": "MaO-fBvyIc8f"
      },
      "cell_type": "code",
      "source": [
        "pol_sarsa = {key: np.argmax(SARSA_Policy(key)) for key in Q_SARSA.keys()}\n",
        "print(\"SARSA Learning Policy\")\n",
        "plot_policy(pol_sarsa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2b02bf02cec99bcc880fcf19b8637edcad7c2191",
        "id": "pI5dd5vaIc8f"
      },
      "cell_type": "markdown",
      "source": [
        "**4.2 Q-Learning: Off-Policy TD**"
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "dc02d75fb5018cdc480504e1157b890cb2b95ef3",
        "id": "ztmfWs-eIc8f"
      },
      "cell_type": "code",
      "source": [
        "def off_pol_TD_Q_learn(env, episodes, epsilon, alpha, gamma):\n",
        "    \"\"\"\n",
        "    Off-Policy TD Q-Learning Method\n",
        "\n",
        "    Args:\n",
        "        env: OpenAI gym environment.\n",
        "        episodes: Number of episodes to sample.\n",
        "        epsilon: Probability of selecting random action instead of the 'optimal' action\n",
        "        alpha: Learning Rate\n",
        "        gamma: Gamma discount factor\n",
        "\n",
        "\n",
        "    Returns:\n",
        "        A tuple (Q, policy).\n",
        "        Q is a dictionary mapping state -> action values.\n",
        "        policy is a function that takes an observation as an argument and returns\n",
        "        action probabilities.\n",
        "    \"\"\"\n",
        "    # Initialise a dictionary that maps state -> action values\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "    # The policy we're following\n",
        "    pol = create_epsilon_greedy_action_policy(env,Q,epsilon)\n",
        "    for i in range(1, episodes + 1):\n",
        "        if i% 1000 == 0:\n",
        "            print(\"\\rEpisode {}/{}.\".format(i, episodes), end=\"\")\n",
        "            clear_output(wait=True)\n",
        "        curr_state = env.reset()\n",
        "        while True:\n",
        "            probs = pol(curr_state)   #get epsilon greedy policy\n",
        "            curr_act = np.random.choice(np.arange(len(probs)), p=probs)\n",
        "            next_state,reward,done,_ = env.step(curr_act)\n",
        "            next_act = np.argmax(Q[next_state])\n",
        "            td_target = reward + gamma * Q[next_state][next_act]\n",
        "            td_error = td_target - Q[curr_state][curr_act]\n",
        "            Q[curr_state][curr_act] = Q[curr_state][curr_act] + alpha * td_error\n",
        "            if done:\n",
        "                break\n",
        "            curr_state = next_state\n",
        "    return Q, pol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "0f89a920818dbdadd8cdda6656fba274913b753d",
        "id": "MA7ANoLOIc8f"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('Blackjack-v0')\n",
        "env.reset()\n",
        "Q_QLearn,QLearn_Policy = off_pol_TD_Q_learn(env, 500000, 0.1, 0.1,0.95)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "07d7ed81786730b9c9933832376e45d110191bf5",
        "id": "I4vWjI5ZIc8f"
      },
      "cell_type": "code",
      "source": [
        "#Payoff for Off-Policy Q-Learning Trained Policy\n",
        "env.reset()\n",
        "calc_payoffs(env,1000,1000,QLearn_Policy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "80181d13b8862467708e71b4e5a14cebc5bfbd71",
        "id": "J0xFVt1KIc8f"
      },
      "cell_type": "code",
      "source": [
        "pol_QLearn = {key: np.argmax(QLearn_Policy(key)) for key in Q_QLearn.keys()}\n",
        "print(\"Off-Policy Q Learning Policy\")\n",
        "plot_policy(pol_QLearn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3d08794f92056d0f5db5fe7ffd29b6c7151600b1",
        "id": "LqzJTAr7Ic8f"
      },
      "cell_type": "markdown",
      "source": [
        "**5. Summary**"
      ]
    },
    {
      "metadata": {
        "_uuid": "76630e2e7fa78549b5b74bad445815921b56dc85",
        "id": "3XqiN3lYIc8f"
      },
      "cell_type": "markdown",
      "source": [
        "We can see the different policies and payoffs for the basic naive strategy, the Monte-Carlo On-Policy trained policy, Monte-Carlo Off-Policy trained policy, TD SARSA (On-Policy) trained policy and TD Q-Learn (Off-Policy) trained policy.\n",
        "\n",
        "The \"best\" policy we trained would be the Monte-Carlo On-Policy policy, with an 'highest' average amount, although it is still negative. So Well, of course the best policy should be to not play at all..."
      ]
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "01306a7411a5d6803d2b017c2672c46561c65797",
        "id": "CgPXIqfyIc8f"
      },
      "cell_type": "code",
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}